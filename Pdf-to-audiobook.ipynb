{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries \n",
    "import PyPDF2\n",
    "import gtts\n",
    "from playsound import playsound\n",
    "import fitz\n",
    "import json\n",
    "import re\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## To get the median\n",
    "def helper_median(lst):\n",
    "    sortedLst = sorted(lst)\n",
    "    lstLen = len(lst)\n",
    "    index = (lstLen - 1) // 2\n",
    "\n",
    "    if (lstLen % 2):\n",
    "        return sortedLst[index]\n",
    "    else:\n",
    "        return (sortedLst[index] + sortedLst[index + 1])/2.0\n",
    "\n",
    "# To get the font median\n",
    "def font_analysis(filename):\n",
    "    doc = fitz.open(filename)\n",
    "    font_size = []\n",
    "    font = []\n",
    "\n",
    "    for page in doc:\n",
    "        sizes =[]\n",
    "        texts = []\n",
    "        bold = []\n",
    "\n",
    "        displayList = page.getDisplayList()\n",
    "\n",
    "        textPage = displayList.getTextPage()\n",
    "\n",
    "        getJsonStr = textPage.extractJSON()\n",
    "\n",
    "        getJson = json.loads(getJsonStr)\n",
    "\n",
    "        for block in getJson['blocks']:\n",
    "            for line in block['lines']:\n",
    "                for span in line['spans']:\n",
    "                    font_size.append(span['size'])\n",
    "                    font.append(span['font'])\n",
    "\n",
    "    median_size = helper_median(font_size)\n",
    "\n",
    "    return median_size\n",
    "\n",
    "\n",
    "# Converting the pdf to text \n",
    "def pdfToText(filename):\n",
    "    doc = fitz.open(filename)\n",
    "    allText = ''\n",
    "    count = 1\n",
    "    pageNo = 1\n",
    "    for page in doc:\n",
    "        sizes =[]\n",
    "        texts = []\n",
    "        bold = []\n",
    "\n",
    "        displayList = page.getDisplayList()\n",
    "\n",
    "        textPage = displayList.getTextPage()\n",
    "\n",
    "        getJsonStr = textPage.extractJSON()\n",
    "\n",
    "        getJson = json.loads(getJsonStr)\n",
    "\n",
    "        for block in getJson['blocks']:\n",
    "            for line in block['lines']:\n",
    "                allText =  allText + '\\n'\n",
    "                for span in line['spans']:\n",
    "                    sentence = span['text']\n",
    "                    font_size = span['size']\n",
    "                    font = span['font'].lower()\n",
    "                    \n",
    "                    if sentence.replace(\" \", \"\").isdigit():\n",
    "                        continue\n",
    "                    \n",
    "#                     font_median = font_analysis(filename)\n",
    "                    \n",
    "#                     if font_size > 20 and 'bold' in font:\n",
    "#                         allText += f'Title {count}' + '\\n' + sentence\n",
    "#                         count += 1\n",
    "#                         continue\n",
    "                        \n",
    "                    allText += sentence\n",
    "        pageNo += 1\n",
    "        \n",
    "    return allText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CNN model contain\n",
      "a large number of zeros. Recent studies in [,,] have shown\n",
      "that these feature maps can be compressed in DRAM by only storing\n",
      "non-zero values and the associated indices to signi￿cantly reduce\n",
      "the memory bandwidth usage. Unfortunately, this optimization\n",
      "leaks the number of zero-valued pixels pruned by the activation\n",
      "function, which can be leveraged to infer the ratio between each\n",
      "weight and the bias value. To the best of our knowledge, this paper\n",
      "represents the ￿rst study on reverse engineering of convolutional\n",
      "neural network models on hardware accelerators, especially in\n",
      "the context of exploiting the side channel through memory access\n",
      "patterns.\n",
      "The rest of the paper is organized as follows: Section 2 de￿nes\n",
      "the assumed threat model; Sections 3 and 4 present two reverse-\n",
      "engineering attacks on the structure and the weights of a CNN\n",
      "model and evaluate the e￿ectiveness of the proposed attacks; Sec-\n",
      "tion 5 discusses the related work, and Section 6 concludes the paper.\n",
      "\n",
      "THREAT MODEL\n",
      "Figure 1 shows a typical CNN inference accelerator architecture\n",
      "that is used in this study. In order to ￿t the input feature maps (IFMs)\n",
      "and ￿lters in the on-chip bu￿ers, IFMs and ￿lters are partitioned\n",
      "into small tiles. Then, the convolution operation is performed over\n",
      "Figure 2: Threat model.\n",
      "all the tiles sequentially. For each tile, the accelerator receives in-\n",
      "structions from the host CPU, reads an IFM tile and corresponding\n",
      "weights from an o￿-chip DRAM into on-chip bu￿ers, performs\n",
      "matrix multiplications and accumulations in the processing ele-\n",
      "ment (PE) array, and store the intermediate results back to on-chip\n",
      "bu￿ers. After computing over all tiles, the accelerator combines the\n",
      "intermediate results and writes an output feature map (OFM) back\n",
      "to DRAM after activation and pooling. The shared cache between\n",
      "the accelerator and CPU is not considered as part of the architecture.\n",
      "The reasons are threefold: (1) the data locality has been exploited\n",
      "by the on-chip bu￿ers; (2) the shared cache is not able to hold all\n",
      "feature maps (FMAPs) and weights; (3) using shared cache to store\n",
      "FMAPs and weights can signi￿cantly degrade the performance of\n",
      "other applications, which are sharing the cache. The FMAPs and\n",
      "weights are typically stored in DRAM, while the intermediate re-\n",
      "sults are kept in on-chip bu￿ers. After ￿nishing the computation of\n",
      "all layers in a forward-propagation fashion, the accelerator returns\n",
      "the probabilities of each class as the classi￿cation result.\n",
      "Figure 2 illustrates the threat model, which captures the common\n",
      "protection capabilities provided by today’s secure processor tech-\n",
      "nologies such as Intel SGX []. The internal operations and state\n",
      "of the CNN accelerator cannot be directly observed or changed\n",
      "by an adversary. The CNN accelerator encrypts feature maps (in-\n",
      "put/output of each layer) and weights in DRAM so that their values\n",
      "are protected. However, the adversary can control inputs to the\n",
      "accelerator and observe the address and the type (read or write) for\n",
      "each o￿-chip memory access. The above threat model represents\n",
      "what is typically employed for today’s secure processors and the\n",
      "level of protection that can be implemented with low overhead. For\n",
      "example, Intel SGX is designed to provide an isolated and protected\n",
      "execution environment for a security-critical program, even when\n",
      "an operating system is untrusted or a system is physically exposed.\n",
      "In such systems, the internal operations and state of a program are\n",
      "protected, although inputs and outputs are still exposed to an adver-\n",
      "sary. Memory accesses can be directly observed through physical\n",
      "probing of a memory bus or inserting hardware Trojan. A compro-\n",
      "mised OS can also observe memory accesses through side channels\n",
      "such as page faults and cache con￿icts [] or by repeated reading\n",
      "memory to detect changes [10].\n",
      "The objective of the reverse-engineering attacks studied in this\n",
      "paper is to construct a duplicated CNN model that has comparable\n",
      "accuracy to the target model by observing the hardware accelerator.\n",
      "This paper studies two di￿erent reverse-engineering attacks on\n",
      "CNNs. The ￿rst attack aims to reverse engineer a network structure\n",
      "(Section 3). The second attack aims to ￿nd weight values (Section 4).\n",
      "Table 1 lists the assumptions made by each attack.\n",
      "\n",
      "STRUCTURE REVERSE ENGINEERING\n",
      "In this section, we discuss how a convolutional neural network\n",
      "structure can be identi￿ed based on the memory access patterns.\n",
      "Reverse engineering attacks on\n",
      "Assumptions\n",
      "Structure\n",
      "(Section 3)\n",
      "Weights\n",
      "(Section 4)\n",
      "Observe memory access patterns\n",
      "Y\n",
      "y\n",
      "Observe the input value\n",
      "N\n",
      "Y\n",
      "Control the input value\n",
      "N\n",
      "Y\n",
      "Possess training data\n",
      "Y\n",
      "N\n",
      "Know the network structure\n",
      "/\n",
      "Y\n",
      "Table 1: Assumptions for each attack – Y: Yes; y: only write\n",
      "accesses need to be visible; N: No; (/): not applicable.\n",
      "Layer parameter\n",
      "De￿nition\n",
      "WI F M/OF M\n",
      "Width of the input/output feature map\n",
      "DI F M/OF M\n",
      "Depth of the input/output feature map\n",
      "Fcon�/pool\n",
      "Width of the conv/pooling ￿lter\n",
      "Scon�/pool\n",
      "Stride of the conv/pooling ￿lter\n",
      "Pcon�/pool\n",
      "Number of pixels padded in the conv/pooling layer\n",
      "P\n",
      "Indicate the existence of the pooling layer\n",
      "Table 2: Parameters to de￿ne a CNN structure.\n",
      "3.1\n",
      "Attack Methodology\n",
      "In order to construct a neural network, an adversary needs to know\n",
      "the number of layers, parameters for each layer, and connections\n",
      "among layers. A typical CNN uses a simple sequential connection\n",
      "only between consecutive layers. More recent proposal [] intro-\n",
      "duces an additional bypass connection among layers.\n",
      "We show that memory access patterns relatively easily reveal the\n",
      "overall layer structure through read-after-write (RAW) dependency.\n",
      "During the CNN inference, the RAW dependency on FMAPs must\n",
      "be preserved by the accelerator, regardless of its micro-architecture\n",
      "details and data reuse strategies. The OFMs are written by a pre-\n",
      "ceding layer and is read as the IFMs by the following layer. Since\n",
      "FMAPs are stored o￿-chip, this RAW dependency is re￿ected in\n",
      "the memory trace and visible to the adversary as a write followed\n",
      "by a read on the same memory address. These RAW dependencies\n",
      "can be used by the adversary to identify the boundary as well as\n",
      "the connections between layers. More concretely, the beginning of\n",
      "a new convolutional/fully connected layer is revealed by the ￿rst\n",
      "read access on a memory address that was previously written.\n",
      "Once the layer boundaries are identi￿ed, the adversary needs\n",
      "to further reverse engineer the key parameters of each layer. The\n",
      "￿rst step towards this goal is to distinguish memory accesses to\n",
      "￿lters, IFM, and OFM. Since the ￿lters are read-only and not up-\n",
      "dated during the inference, the adversary can di￿erentiate memory\n",
      "accesses to ￿lters from those accessing FMAPs. The read/write\n",
      "operations on IFM and OFM can also be distinguished since they\n",
      "have di￿erent access patterns. During the computation within a\n",
      "layer, memory locations holding OFM will only be written, typi-\n",
      "cally once. In contrast, if the adversary observes a read access on\n",
      "an address written in the previous layer before, this read must be\n",
      "for IFM. FMAPs and ￿lters are stored as arrays in memory, which\n",
      "means that each is stored in its own contiguous memory locations.\n",
      "Therefore, an adversary can infer the sizes of IFM (SIZEI F M), OFM\n",
      "(SIZEOF M), and ￿lters (SIZEF LT R) of each layer by observing the\n",
      "memory locations accessed for each data structure within a layer.\n",
      "So far, we have shown that the memory access pattern of a\n",
      "CNN accelerator reveals the number of layers, data dependencies\n",
      "(connections) among layers, the size of the IFM, OFM, and ￿lters\n",
      "for each layer in the target CNN model. However, not all operations\n",
      "\n",
      "in the neural network are explicitly revealed by the memory access\n",
      "pattern. For example, a CNN performs an activation operation\n",
      "after each convolution followed by an optional pooling operation.\n",
      "These three operations are often merged and performed together\n",
      "as a single layer in CNN accelerator for e￿ciency. As a result,\n",
      "the internal outputs of these three operations are invisible to the\n",
      "adversary.\n",
      "There are 11 structural parameters that the adversary needs\n",
      "to determine for each layer in order to fully de￿ne the network\n",
      "structure, as listed in Table 2. The problem of identifying each layer\n",
      "structure can be formulated as solving the 11 integer parameters\n",
      "with the following equations:\n",
      "SIZ EI F M = W \n",
      "I F M ⇥ DI F M\n",
      "(1)\n",
      "SIZ EO F M = W \n",
      "O F M ⇥ DO F M\n",
      "(2)\n",
      "SIZ EF LT R = F \n",
      "con� ⇥ DI F M ⇥ DO F M\n",
      "(3)\n",
      "WOF M =\n",
      "(WI F M �Fcon� +Pcon� )\n",
      "Scon�\n",
      "+ + (Ppool � Fpool ) ⇥ P\n",
      "Spool ⇥ P + ¯P\n",
      "+ P\n",
      "(4)\n",
      "Scon�  Fcon�  WI F M\n",
      "\n",
      "(5)\n",
      "Spool  Fpool  (WI F M � Fcon� + PI F M )\n",
      "Scon�\n",
      "+\n",
      "(6)\n",
      "Pcon� < Fcon�\n",
      "(7)\n",
      "Ppool < Fpool\n",
      "(8)\n",
      "Equations (1)-(3) are derived from the size of the FMAPs and\n",
      "weights revealed by the memory access patterns. Equation (4) ex-\n",
      "presses the relationship between the width of IFM and OFM. The\n",
      "inequalities are based on the following practical considerations.\n",
      "First, Fcon� and Fpool should be no less than the stride to cover\n",
      "all the pixels in IFM. Otherwise, some pixels in IFM are not con-\n",
      "nected with the weights and become redundant. Second, the ￿lters\n",
      "should be smaller compared to the width of IFM. Lastly, Pcon� and\n",
      "Ppool should be smaller than Fcon� and Fpool respectively because\n",
      "the convolutional/pooling ￿lter operating on zero-valued pixels is\n",
      "equivalent to adding zero-padding for the next layer.\n",
      "In our threat model, the adversary also has the knowledge of\n",
      "input and output of the accelerator which reveals the WI F M and\n",
      "DI F M of the ￿rst layer and the DOF M of the last layer. Although\n",
      "there is no previous write on the IFM of the ￿rst layer, the SIZEI F M\n",
      "can be calculated using Equation 1 and thus distinguished from the\n",
      "weights. Moreover, the WOF M of the last layer is one since there is\n",
      "exactly one score for each class. With these additional constraints,\n",
      "adversary can enumerate all possible parameters that satisfy Equa-\n",
      "tion (1)-(8) for the ￿rst layer and feed the possible WOF M and\n",
      "DOF M as the constraints for the second layer. Through enumerat-\n",
      "ing possible parameters layer-by-layer, the network structure is the\n",
      "combination of possible con￿gurations of each layer.\n",
      "In order to further reduce the number of possible structures,\n",
      "the execution time of each layer is measured by recording the\n",
      "number of clock cycles between the boundaries and introduced\n",
      "as an additional constraint. The number of MAC operations of a\n",
      "speci￿c layer can be computed with the layer parameters (# of MACs\n",
      "= W \n",
      "OF M ⇥ DOF M ⇥ Fcon� ⇥ DI F M). Given that the inference of\n",
      "most CNN models is compute-bound, we assume that the execution\n",
      "time is roughly proportional to the number of MAC operations.\n",
      "Thus, the execution time ratio between layers should be consistent\n",
      "with the ratio of MAC operations for the correct con￿guration.\n",
      "Once a small number of candidate structures are identi￿ed through\n",
      "the reverse engineering, an adversary can pick the best structure\n",
      "Networks\n",
      "LeNet\n",
      "ConvNet\n",
      "AlexNet\n",
      "SqueezeNet\n",
      "# of layers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# of possible structures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table 3: Possible structures for di￿erent networks.\n",
      "Figure 3: Memory access pattern of the FPGA accelerator.\n",
      "by training and comparing the accuracy. Algorithm 1 summarizes\n",
      "the overall structure reverse-engineering attack procedure.\n",
      "Algorithm 1 Steps to uncover the possible CNN structures\n",
      "1: Identify layer boundaries by observing the RAW dependency\n",
      "on FMAPs\n",
      "2: Record the execution time of each layer and calculate the\n",
      "SIZEI F M, SIZEOF M, and SIZEF LT R based on the memory ac-\n",
      "cess pattern\n",
      "3: Find possible con￿gurations for each layer with the constraints\n",
      "stated in Equations (1)-(8)\n",
      "4: Filter out the con￿gurations where the number of MAC opera-\n",
      "tions and the execution time do not match\n",
      "5: List valid combination of layers as possible structure which\n",
      "satis￿es (WOF Mi = WI F Mi+) ^ (DOF Mi = DI F Mi+)\n",
      "3.2\n",
      "Case Studies\n",
      "To evaluate the proposed structure reverse-engineering attack,\n",
      "we performed case studies on popular CNN models: an 8-layer\n",
      "AlexNet [] and a more recent 18-layer SqueezeNet []. We also\n",
      "studied other smaller networks such as LeNet and ConvNet. The\n",
      "number of possible structures identi￿ed by the proposed attack is\n",
      "summarized in Table 3.\n",
      "We implemented an FPGA accelerator for AlexNet using Vivado\n",
      "HLS and performed the attack by inserting a hardware Trojan to\n",
      "collect the memory trace of the accelerator. The layer boundaries\n",
      "are identi￿ed by observing the RAW dependency on FMAPs, as de-\n",
      "picted in Figure 3. Table 4 lists the possible con￿gurations for each\n",
      "layer in AlexNet. A total of 24 valid combinations are uncovered\n",
      "by applying the proposed method on the FPGA prototype. A pool-\n",
      "ing layer, if exists, is considered as part of the convolutional layer.\n",
      "AlexNet consists of ￿ve CONV layers and three FC layers. The FC\n",
      "layers are not listed in the Table 4 because they use the largest\n",
      "possible ￿lter size (W \n",
      "I F M ⇥ DI F M ⇥ DOF M) and always have a\n",
      "unique con￿guration with respect to the Equations (1)-(8). The orig-\n",
      "inal AlexNet structure consists of CONV, CONV, CONV,\n",
      "CONV 4, and CONV. The top-1 validation accuracies of 24 possi-\n",
      "ble structures are shown in Figure 4. The original AlexNet achieves\n",
      "the fourth highest accuracy (57.3%). The attack also found three\n",
      "other network structure, which are slightly di￿erent from the orig-\n",
      "inal AlexNet and have higher accuracy. The best structure achieves\n",
      "12.3% higher accuracy than the worst one, showing the importance\n",
      "of a good network structure.\n",
      "\n",
      "La�er\n",
      "WI F M\n",
      "DI F M\n",
      "WOF M\n",
      "DOF M\n",
      "Fcon�\n",
      "Scon�\n",
      "Pcon�\n",
      "Fpool\n",
      "Spool\n",
      "Ppool\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table 4: Possible AlexNet layer con￿gurations – N/A indi-\n",
      "cates that there is no pooling operation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Possible AlexNet structures\n",
      "Top-1 validation accuracy (%)\n",
      "Other possible network structures\n",
      "Original AlexNet structure\n",
      "Figure 4: Top-1 validation accuracy among 24 possible struc-\n",
      "tures for AlexNet.\n",
      "In addition to AlexNet, we also studied reverse engineering of\n",
      "a more recent state-of-art CNN structure. Compared to AlexNet,\n",
      "two new trends emerged in network structure designs in the past\n",
      "few years. GoogLeNet [] proposed concatenating multiple con-\n",
      "volution ￿lters with di￿erent Fcon� as a module and using this\n",
      "module repeatedly to form the network. ResNet [] introduced a\n",
      "bypass connection between two non-adjacent layers. SqueezeNet\n",
      "adopted both of these structural changes and achieves an accuracy\n",
      "comparable to that of AlexNet while using 50x less weights.\n",
      "We use SqueezeNe as an example to demonstrate the e￿ective-\n",
      "ness of the proposed attack on a more recent network. SqueezeNet\n",
      "consists of two CONV layers and eight ￿re modules and each ￿re\n",
      "module is made of concatenating one 1x1 and two 3x3 convolutional\n",
      "￿lters. The 1x1 CONV layer uses small DOF M to squeeze the size of\n",
      "FMAPs. The OFM of the 1x1 CONV layer feeds into two 3x3 CONV\n",
      "layers in parallel and the OFMs of the two following layers are con-\n",
      "catenated along the depth dimension as the ￿nal OFM of the ￿re\n",
      "module. Compared to normal feed-through structure, SqueezeNet\n",
      "also introduces three bypass paths connecting non-adjacent ￿re\n",
      "modules. The bypass path is combined with the feed-through path\n",
      "by applying element-wise additions on the OFMs.\n",
      "To our best knowledge, there is no accelerator design that uses\n",
      "dedicated hardware for the ￿re module because it can be imple-\n",
      "mented using existing CONV layer accelerators. The three CONV\n",
      "layers will be executed sequentially. The IFM is convolved with 1x1\n",
      "CONV ￿lter ￿rst and the OFM of this 1x1 convolution is then con-\n",
      "voluted with 1x1 and 3x3 CONV ￿lters in series. Assuming that the\n",
      "three convolution operations are executed sequentially on a CNN\n",
      "accelerator, the adversary can observe the RAW dependency be-\n",
      "tween layers which reveals the structure of the ￿re module. Instead\n",
      "of having RAW dependency between the neighboring layers, the\n",
      "N ⇥N ￿lter is an abbreviation of N ⇥N ⇥DOF M ￿lter. And N ⇥N convolution\n",
      "stands for performing convolution with N ⇥ N ⇥ DOF M ￿lter.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "Figure 5: The top-5 validation accuracy of nine possible\n",
      "structures for SqueezeNet.\n",
      "bypass path introduces extra RAW dependency across non-adjacent\n",
      "layers. The easiest way of implementing this bypass function is\n",
      "to wait until two OFMs from two paths are both ready, then load\n",
      "them from memory and perform the element-wise additions. This\n",
      "method is adopted by Ca￿e and TensorFlow. In both frameworks, a\n",
      "separate element-wise layer is introduced to realize the bypass func-\n",
      "tion. Assuming that the CNN accelerator follows the same strategy,\n",
      "the bypass path can also be detected from the RAW dependency in\n",
      "memory accesses.\n",
      "We performed the proposed attack on SqueezeNet, and found\n",
      "that there are nine possible con￿gurations for CONV1 layer, 12\n",
      "possible con￿gurations for the ￿re modules, and two possible con-\n",
      "￿gurations for the CONV10 layer. Theoretically, there exists 329\n",
      "valid combinations, which makes it expensive to test all valid com-\n",
      "binations to identify the network structure. However, large CNNs\n",
      "are typically constructed in a modular fashion, where the same\n",
      "building block is reused in order to reduce the complexity. If we\n",
      "assume that the structures of all ￿re module are identical, there\n",
      "exists only one valid con￿guration for the ￿re module and CONV10\n",
      "layer. The number of possible CNN structure candidates is reduced\n",
      "to nine. This example shows that the number of possible structures\n",
      "does not necessarily grow exponentially with the number of layers.\n",
      "The time to search for the best network structure among the\n",
      "possible candidates can be reduced by using short training to\n",
      "quickly ￿lter out unpromising candidates with low classi￿cation\n",
      "accuracy. For example, CNN training is often performed using\n",
      "many epochs, where one epoch goes through the entire training\n",
      "dataset once. Figure 5 shows the accuracy of possible candidates for\n",
      "SqueezeNet when only three epochs were used for training. The\n",
      "original SqueezeNet proposed using around 70 epochs for training.\n",
      "There is a signi￿cant di￿erence in accuracy among the structure\n",
      "candidates even with a small number of epochs, suggesting that\n",
      "unpromising network structures can be quickly ￿ltered out.\n",
      "\n",
      "REVERSE ENGINEERING WEIGHTS\n",
      "EXPLOITING ZERO PRUNING\n",
      "In this section, we introduce an attach to obtain information on\n",
      "weights when an optimization technique is used to prune zeros in\n",
      "FMAPs. This attack also exploits information leaks through memory\n",
      "access patterns of the CNN accelerator. However, only the write\n",
      "accesses need to be observable for the attack.\n",
      "In recent years, multiple CNN accelerator designs observed that\n",
      "the ReLU function leads to a large number of zeros in the feature\n",
      "maps and proposed to dynamically prune those zeros [,,].\n",
      "\n",
      "(a) 1x1 CONV with stride 1.\n",
      "(b) n ⇥ n CONV with stride 1.\n",
      "Figure 6: 1x1 and n ⇥ n convolution.\n",
      "Such zero pruning technique is shown to be quite e￿ective, reduc-\n",
      "ing convolution operations by 40% on average without a￿ecting the\n",
      "classi￿cation accuracy. The zero pruning also reduces the number\n",
      "of memory accesses by only writing and reading non-zero values,\n",
      "for example using a run-length encoding. However, the dynamic\n",
      "zero pruning reveals the number of zeros in OFM. An adversary\n",
      "who can observe memory accesses can detect when the number\n",
      "of zeros changes. We show that this dynamic behavior leaks in-\n",
      "formation about the weights of the CNN model. The adversary\n",
      "cannot precisely recover all weights, but can recover substantial\n",
      "information so that each weight can be expressed as a function of\n",
      "one bias value.\n",
      "4.1\n",
      "Attack Methodology\n",
      "In a CNN accelerator, di￿erent inputs lead to a di￿erent number of\n",
      "non-zero pixels in the OFM of each layer. With the dynamic zero\n",
      "pruning, the adversary can observe the changes in the number of\n",
      "non-zero pixels in OFM. If the activation function f maps negative\n",
      "values to zero as in ReLU, the change in the number of zeros in OFM\n",
      "actually reveals when a pixel crosses the zero boundary. The value\n",
      "of each pixel� in OFM can be expressed as a function of the value of\n",
      "pixels x in IFM, weights w, and a bias b, that is � = f (Õ\n",
      "i wi ·xi +b).\n",
      "Therefore, if an adversary can slowly change the pixel values in\n",
      "IFM and observe the number of zeros in OFM, the adversary can\n",
      "e￿ectively ￿nd out when the value of a pixel in OFM becomes zero\n",
      "(Õ\n",
      "i wi ·xi +b = 0). If the adversary can ￿nd out which pixel is zero,\n",
      "then one can write a set of linear equations for wi and b given that\n",
      "the values of input xi are known.\n",
      "Unfortunately, the dynamic zero pruning only leaks the number\n",
      "of zero-valued pixels in OFM while the exact locations of those\n",
      "pixels along W , H, and D dimensions remain unknown. To solve\n",
      "the problem, an adversary can provide carefully crafted inputs to\n",
      "the accelerator. Reverse engineering the weights of 1x1 CONV, 2x2\n",
      "CONV, and FC layers are more straightforward compared to the\n",
      "general case with a larger ￿lter (Fcon� >). We ￿rst illustrate our\n",
      "approach using a special case when Fcon� = 1. Then, the approach\n",
      "is extended to work with any CONV ￿lter size.\n",
      "Figures 6a shows an example where a 3 ⇥ 3 IFM is convolved\n",
      "with a 1 ⇥ 1 ￿lter. In the 1x1 convolution, a single weight is shared\n",
      "by all WI F M ⇥ HI F M pixels on the same 2-D plane. In other words,\n",
      "the product of any pixel in IFM and the weight only a￿ect one pixel\n",
      "in the corresponding OFM. This relationship can be expressed by\n",
      "�i,j = xi,j ·w +b. Thus, for a 1x1 CONV layer, an adversary can vary\n",
      "the value of one speci￿c pixel in IFM (denoted as a variable x) and\n",
      "set all other pixels to be 0 as depicted in Figure 6a. By monitoring the\n",
      "number of non-zeros in OFM, the adversary can easily determine if\n",
      "w · x + b > 0 holds for a given x. Through a binary search on the\n",
      "value of x, the adversary will be able to ￿nd a maximum xH and a\n",
      "minimum xL, which satisfy:\n",
      "(w · xH + b) ^ (w · xL + b >)\n",
      "(9)\n",
      "Then (xH +xL)/2 can be estimated to be the input that produces\n",
      "zero (� = 0), which we subsequently refer to as a zero crossing\n",
      "point. It can be used to approximate the value of b/w.\n",
      "Figure 6b illustrates the general case of applying a n ⇥ n CONV\n",
      "￿lter on an IFM (WI F M >n). The number of connections be-\n",
      "tween each pixel and the weights is shown in the IFM. Having a\n",
      "connection with the weight wi,j means that pixel contributes to\n",
      "the corresponding output pixel �i,j. For instance, pixel x, at the\n",
      "top-left corner is only connected with w, and contributes to �,\n",
      "whereas pixel x, is connected with w, and w,, contributing\n",
      "to both �, and �,. xn,n, which is a pixel on the nth column and\n",
      "row, is connected with all n weights in the 2-D ￿lter.\n",
      "As the weights closer to the corner contributes to a less numbers\n",
      "of output pixels, we can iteratively ￿nd the ratio between each\n",
      "￿lter weight and the bias. For example, performing a binary search\n",
      "on x, reveals the approximate value of b/w,. Then, a search\n",
      "on x, while setting all other pixels to zero gives two regions\n",
      "where the number of zero-valued outputs changes between 0 and 2.\n",
      "Each one of these zero-crossing points is the approximate value of\n",
      "b/w, or b/w,. Given that b/w, is already known, an attacker\n",
      "can determine which value corresponds to b/w,. Algorithm 2\n",
      "describes the proposed method for discovering all b/wi,j on the\n",
      "same 2-D plane. Note that, if wi,j = 0, no x and x will satisfy\n",
      "Equation (9) which means no zero-crossing point can be found\n",
      "during the search. Therefore, zero-valued weights can be identi￿ed\n",
      "from missing zero-crossing points.\n",
      "Algorithm 2 Reverse engineering CNN weights\n",
      "1: for i = 0 to Fcon� � do\n",
      "2:\n",
      "for j = 0 to Fcon� � do\n",
      "3:\n",
      "Set all inputs except xi,j to zero.\n",
      "4:\n",
      "Find (xH + xL)/2 values where the number of non-zero\n",
      "output pixels change (zero crossing points).\n",
      "5:\n",
      "Set the new zero-crossing point as b/wi,j.\n",
      "6:\n",
      "end for\n",
      "7: end for\n",
      "A convolutional layer may be followed by a maximum or average\n",
      "pooling layer. These two layers are usually merged in a CNN ac-\n",
      "celerator to avoid unnecessary o￿-chip memory accesses. Without\n",
      "the pooling layer, varying x, while keeping other inputs to be\n",
      "zero leads to two zero-crossing points. However, when there is a\n",
      "max pooling layer where a 2x2 pooling window is concatenated\n",
      "with a n ⇥ n CONV layer, the search on x, only gives one zero-\n",
      "crossing point because both non-zero outputs are replaced by the\n",
      "same maximum value of w, · x, and w, · x,. If w, < w,,\n",
      "this zero-crossing point corresponds to b/w,. In order to also ￿nd\n",
      "b/w,, we need to keep both x, and x, as non-zero variables as\n",
      "in the following equation:\n",
      "(\n",
      "�, = max\n",
      "�\n",
      "w, · x, + w, · x,, w, · x,\n",
      " \n",
      "�, = w, · x, + b\n",
      "(10)\n",
      "Given that the b/w, has already been inferred, the attacker can\n",
      "￿nd the value for x, which satis￿es �,  0. Then performing\n",
      "binary search on x, to ￿nd the zero crossing point for w, ·\n",
      "The same bias is shared by all the weights in one ￿lter.\n",
      "\n",
      "Figure 7: Weight/Bias ratio of the CONV1 layer in AlexNet.\n",
      "x, + w, · x, + b gives\n",
      "b\n",
      "w,\n",
      "=\n",
      "�x,\n",
      " + x, ·w, /b . This modi￿ed\n",
      "algorithm can be generalized to be compatible with any k ⇥ k\n",
      "max pooling window. With a 2x2 average pooling layer, the same\n",
      "approach can be applied with the modi￿ed equation in Equation (11)\n",
      "and b/w, can be expressed with\n",
      "�bx,\n",
      "b + x, ·w,\n",
      ".\n",
      "(\n",
      "�, = w,·x,+b+w,·x,+b\n",
      "\n",
      "�, = w,·x,+b\n",
      "\n",
      "(11)\n",
      "The proposed attack on the dynamic zero pruning scheme al-\n",
      "lows each weight to be expressed as a function of the bias. This\n",
      "signi￿cantly reduces the entropy in weights as only the bias is left\n",
      "to be unknown. This unknown bias cannot be determined only\n",
      "from the information leak through the number of non-zero pixels.\n",
      "The changes in the number of non-zero pixels in OFM provides\n",
      "n unique equalities, one for each pixel for a n ⇥ n convolutional\n",
      "￿lter. Yet, there exists n + 1 variables for the ￿lter (n weights\n",
      "and one bias value). In order to determine the exact weights and\n",
      "the bias, an adversary needs to leverage additional information.\n",
      "For example, recent accelerator designs [,] proposed to use a\n",
      "tunable threshold function in place of the ReLU function in order to\n",
      "prune more pixels with small values and thus improves e￿ciency.\n",
      "However, if this non-zero threshold value is known and can be\n",
      "adjusted, an adversary can set the input to be all zeros and vary\n",
      "the threshold to ￿nd the bias values. Since the ratio between each\n",
      "weight and bias is known, this optimization enables an adversary\n",
      "to fully recover the weight and bias values.\n",
      "4.2\n",
      "Case Study\n",
      "We demonstrate the proposed attack on the ￿rst layer of a com-\n",
      "pressed AlexNet model [], which contains zero-valued weights.\n",
      "The inferred w/b of all 96 ￿lters are shown in Figure 7. The zero-\n",
      "valued weights are detected and the maximum di￿erence between\n",
      "the inferred and the original ratio is less than 2�.\n",
      "\n",
      "RELATED WORK\n",
      "While this work represents the ￿rst concrete study on exploiting\n",
      "information leaks through memory access patterns in the context\n",
      "of reverse engineering CNNs, hiding information leaks through\n",
      "memory access patterns in general is a well-studied problem. In\n",
      "particular, oblivious RAM (ORAM) algorithms [] provide a strong\n",
      "theoretical guarantee for obfuscating the memory accesses. ORAM\n",
      "can be used to prevent attacks proposed in this paper. However,\n",
      "even an e￿cient hardware implementation [] of the state-of-the-\n",
      "art ORAM algorithm [] signi￿cantly increases the number of\n",
      "memory accesses, and likely to result in signi￿cant overhead for\n",
      "the CNN inference, which is a memory-intensive task.\n",
      "Membership inference attack [] proposed constructing shadow\n",
      "models to identify whether a input belongs to the original training\n",
      "dataset or not. This attack relies on training the shadow models\n",
      "when the network structure is already known. The proposed struc-\n",
      "ture reverse-engineering attack can be used to enable such attacks.\n",
      "\n",
      "CONCLUSION\n",
      "This paper studies potential vulnerabilities in CNN accelerators in\n",
      "the context of stealing a CNN model. The study shows that both\n",
      "the network structure and weights of a CNN model can be revealed\n",
      "through the memory access patterns and the input/output of the\n",
      "accelerator even when no internal access is allowed and all o￿-\n",
      "chip data are encrypted. Our ￿ndings highlight the need for hiding\n",
      "memory access patterns for CNN accelerators. The study also shows\n",
      "that performance optimization can lead to an unexpected security\n",
      "vulnerability and needs to be carefully reviewed.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "This work was partially sponsored by NSF award CNS-1618275,\n",
      "Semiconductor Research Corporation under Task 2686.001, and a\n",
      "GPU donation from NVIDIA Corporation.\n",
      "REFERENCES\n",
      "[1] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright\n",
      "Jerger, and Andreas Moshovos. Cnvlutin: Ine￿ectual-neuron-free Deep Neural\n",
      "Network Computing. In ISCA, 2016.\n",
      "[2] Victor Costan and Srinivas Devadas. Intel SGX Explained. IACR Cryptology\n",
      "ePrint Archive, 2016.\n",
      "[3] Christopher W. Fletcher, Ling Ren, Albert Kwon, Marten van Dijk, Emil Stefanov,\n",
      "Dimitrios Serpanos, and Srinivas Devadas. A Low-Latency, Low-Area Hardware\n",
      "Oblivious RAM Controller. In FCCM, 2015.\n",
      "[4] Oded Goldreich and Rafail Ostrovsky. Software Protection and Simulation on\n",
      "Oblivious RAMs. J. ACM, 1996.\n",
      "[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
      "Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets.\n",
      "In NIPS. 2014.\n",
      "[6] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing\n",
      "Deep Neural Network with Pruning, Trained Quantization and Hu￿man Coding.\n",
      "CoRR, 2015.\n",
      "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning\n",
      "for Image Recognition. CoRR, 2015.\n",
      "[8] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J.\n",
      "Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer\n",
      "parameters and <1MB model size. CoRR, 2016.\n",
      "[9] Alex Krizhevsky, Ilya Sutskever, and Geo￿rey E Hinton. ImageNet Classi￿cation\n",
      "with Deep Convolutional Neural Networks. In NIPS. 2012.\n",
      "[10] Lichun Li and Anwitaman Datta. Write-only Oblivious RAM-based Privacy-\n",
      "preserved Access of Outsourced Data. Int. J. Inf. Secur., 2017.\n",
      "[11] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rang-\n",
      "harajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and\n",
      "William J. Dally. SCNN: An Accelerator for Compressed-sparse Convolutional\n",
      "Neural Networks. In ISCA, 2017.\n",
      "[12] B. Reagen, P. Whatmough, R. Adolf, S. Rama, H. Lee, S. K. Lee, J. M. HernÃąndez-\n",
      "Lobato, G. Y. Wei, and D. Brooks. Minerva: Enabling Low-Power, Highly-Accurate\n",
      "Deep Neural Network Accelerators. In ISCA, 2016.\n",
      "[13] Reza Shokri and Vitaly Shmatikov. Privacy-Preserving Deep Learning. In Pro-\n",
      "ceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications\n",
      "Security, CCS, 2015.\n",
      "[14] Reza Shokri, Marco Stronati, and Vitaly Shmatikov. Membership Inference\n",
      "Attacks against Machine Learning Models. CoRR, 2016.\n",
      "[15] Emil Stefanov, Marten van Dijk, Elaine Shi, Christopher Fletcher, Ling Ren,\n",
      "Xiangyao Yu, and Srinivas Devadas. Path ORAM: An Extremely Simple Oblivious\n",
      "RAM Protocol. In CCS, 2013.\n",
      "[16] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,\n",
      "Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-\n",
      "novich. Going Deeper with Convolutions. CoRR, 2014.\n",
      "[17] Yuanzhong Xu, Weidong Cui, and Marcus Peinado. Controlled-Channel Attacks:\n",
      "Deterministic Side Channels for Untrusted Operating Systems. In S&P, 2015.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "\n",
    "\n",
    "## Selecting the file location\n",
    "filelocation = askopenfilename() # open the dialog GUI\n",
    "\n",
    "allText = pdfToText(filelocation)\n",
    "print(allText)\n",
    "# with open(filelocation, \"rb\") as f:  # open the file in reading (rb) mode and call it f\n",
    "#     pdf = pdftotext.PDF(f)  # store a text version of the pdf file f in pdf variable\n",
    "\n",
    "# string_of_text = ''\n",
    "# for text in pdf:\n",
    "#     string_of_text += text\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# final_file = gTTS(text=Alltext, lang='en', slow = False)  # store file in variable\n",
    "# final_file.save(\"Generated Speech.mp3\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "tot_iteration, tot_division = len(allText)//4800, len(allText)/4800\n",
    "\n",
    "# print(leftover - tot_iteration )\n",
    "# print(int(4800*(leftover - tot_iteration)))\n",
    "\n",
    "start = 0 \n",
    "end = 4800\n",
    "count = 1\n",
    "for i in range(0, tot_iteration):\n",
    "    globals()['part%s' % count] = allText[start:end]\n",
    "    count += 1\n",
    "    start = copy.deepcopy(end) \n",
    "    end = end + 4800\n",
    "\n",
    "\n",
    "leftover = int(4800*(leftover - tot_iteration))\n",
    "start = copy.deepcopy(end)\n",
    "end = end + leftover\n",
    "globals()['part%s' % count] = allText[start: end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600\n",
      "36228\n"
     ]
    }
   ],
   "source": [
    "check = ''\n",
    "for i in range(1, tot_iteration+1):\n",
    "    check += globals()['part%s' % 1]\n",
    "    \n",
    "print(len(check))\n",
    "print(len(allText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 5000 characters limit exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"5000 characters limit exceeded.\"\n\tdebug_error_string = \"{\"created\":\"@1601600570.020983000\",\"description\":\"Error received from peer ipv4:142.250.68.10:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"5000 characters limit exceeded.\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7e83db54450d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# voice parameters and audio file type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m response = client.synthesize_speech(\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynthesis_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/google/cloud/texttospeech_v1/services/text_to_speech/client.py\u001b[0m in \u001b[0;36msynthesize_speech\u001b[0;34m(self, request, input, voice, audio_config, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pdf_audio/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 5000 characters limit exceeded."
     ]
    }
   ],
   "source": [
    "from google.cloud import texttospeech\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/hiteshsaaimanancherypanneerselvam/Documents/Development_Projects/pdf_to_audiobook/My First Project-1f1b9b4bc545.json\"\n",
    "\n",
    "\n",
    "# Instantiates a client\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Set the text input to be synthesized\n",
    "synthesis_input = texttospeech.SynthesisInput(text= allText)\n",
    "\n",
    "# Build the voice request, select the language code (\"en-US\") and the ssml\n",
    "# voice gender (\"neutral\")\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
    ")\n",
    "\n",
    "# Select the type of audio file you want returned\n",
    "audio_config = texttospeech.AudioConfig(\n",
    "    audio_encoding=texttospeech.AudioEncoding.MP3\n",
    ")\n",
    "\n",
    "# Perform the text-to-speech request on the text input with the selected\n",
    "# voice parameters and audio file type\n",
    "response = client.synthesize_speech(\n",
    "    input=synthesis_input, voice=voice, audio_config=audio_config\n",
    ")\n",
    "\n",
    "# The response's audio_content is binary.\n",
    "with open(\"output.mp3\", \"wb\") as out:\n",
    "    # Write the response to the output file.\n",
    "    out.write(response.audio_content)\n",
    "    print('Audio content written to file \"output.mp3\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CNN model contain\n",
      "a large number of zeros. Recent studies in [,,] have shown\n",
      "that these feature maps can be compressed in DRAM by only storing\n",
      "non-zero values and the associated indices to signi￿cantly reduce\n",
      "the memory bandwidth usage. Unfortunately, this optimization\n",
      "leaks the number of zero-valued pixels pruned by the activation\n",
      "function, which can be leveraged to infer the ratio between each\n",
      "weight and the bias value. To the best of our knowledge, this paper\n",
      "represents the ￿rst study on reverse engineering of convolutional\n",
      "neural network models on hardware accelerators, especially in\n",
      "the context of exploiting the side channel through memory access\n",
      "patterns.\n",
      "The rest of the paper is organized as follows: Section 2 de￿nes\n",
      "the assumed threat model; Sections 3 and 4 present two reverse-\n",
      "engineering attacks on the structure and the weights of a CNN\n",
      "model and evaluate the e￿ectiveness of the proposed attacks; Sec-\n",
      "tion 5 discusses the related work, and Section 6 concludes the paper.\n",
      "\n",
      "THREAT MODEL\n",
      "Figure 1 shows a typical CNN inference accelerator architecture\n",
      "that is used in this study. In order to ￿t the input feature maps (IFMs)\n",
      "and ￿lters in the on-chip bu￿ers, IFMs and ￿lters are partitioned\n",
      "into small tiles. Then, the convolution operation is performed over\n",
      "Figure 2: Threat model.\n",
      "all the tiles sequentially. For each tile, the accelerator receives in-\n",
      "structions from the host CPU, reads an IFM tile and corresponding\n",
      "weights from an o￿-chip DRAM into on-chip bu￿ers, performs\n",
      "matrix multiplications and accumulations in the processing ele-\n",
      "ment (PE) array, and store the intermediate results back to on-chip\n",
      "bu￿ers. After computing over all tiles, the accelerator combines the\n",
      "intermediate results and writes an output feature map (OFM) back\n",
      "to DRAM after activation and pooling. The shared cache between\n",
      "the accelerator and CPU is not considered as part of the architecture.\n",
      "The reasons are threefold: (1) the data locality has been exploited\n",
      "by the on-chip bu￿ers; (2) the shared cache is not able to hold all\n",
      "feature maps (FMAPs) and weights; (3) using shared cache to store\n",
      "FMAPs and weights can signi￿cantly degrade the performance of\n",
      "other applications, which are sharing the cache. The FMAPs and\n",
      "weights are typically stored in DRAM, while the intermediate re-\n",
      "sults are kept in on-chip bu￿ers. After ￿nishing the computation of\n",
      "all layers in a forward-propagation fashion, the accelerator returns\n",
      "the probabilities of each class as the classi￿cation result.\n",
      "Figure 2 illustrates the threat model, which captures the common\n",
      "protection capabilities provided by today’s secure processor tech-\n",
      "nologies such as Intel SGX []. The internal operations and state\n",
      "of the CNN accelerator cannot be directly observed or changed\n",
      "by an adversary. The CNN accelerator encrypts feature maps (in-\n",
      "put/output of each layer) and weights in DRAM so that their values\n",
      "are protected. However, the adversary can control inputs to the\n",
      "accelerator and observe the address and the type (read or write) for\n",
      "each o￿-chip memory access. The above threat model represents\n",
      "what is typically employed for today’s secure processors and the\n",
      "level of protection that can be implemented with low overhead. For\n",
      "example, Intel SGX is designed to provide an isolated and protected\n",
      "execution environment for a security-critical program, even when\n",
      "an operating system is untrusted or a system is physically exposed.\n",
      "In such systems, the internal operations and state of a program are\n",
      "protected, although inputs and outputs are still exposed to an adver-\n",
      "sary. Memory accesses can be directly observed through physical\n",
      "probing of a memory bus or inserting hardware Trojan. A compro-\n",
      "mised OS can also observe memory accesses through side channels\n",
      "such as page faults and cache con￿icts [] or by repeated reading\n",
      "memory to detect changes [10].\n",
      "The objective of the reverse-engineering attacks studied in this\n",
      "paper is to construct a duplicated CNN model that has comparable\n",
      "accuracy to the target model by observing the hardware accelerator.\n",
      "This paper studies two di￿erent reverse-engineering attacks on\n",
      "CNNs. The ￿rst attack aims to reverse engineer a network structure\n",
      "(Section 3). The second attack aims to ￿nd weight values (Section 4).\n",
      "Table 1 lists the assumptions made by each attack.\n",
      "\n",
      "STRUCTURE REVERSE ENGINEERING\n",
      "In this section, we discuss how a convolutional neural network\n",
      "structure can be identi￿ed based on the memory access patterns.\n",
      "Reverse engineering attacks on\n",
      "Assumptions\n",
      "Structure\n",
      "(Section 3)\n",
      "Weights\n",
      "(Section 4)\n",
      "Observe memory access patterns\n",
      "Y\n",
      "y\n",
      "Observe the input value\n",
      "N\n",
      "Y\n",
      "Control the input value\n",
      "N\n",
      "Y\n",
      "Possess training data\n",
      "Y\n",
      "N\n",
      "Know the network structure\n",
      "/\n",
      "Y\n",
      "Table 1: Assumptions for each attack – Y: Yes; y: only write\n",
      "accesses need to be visible; N: No; (/): not applicable.\n",
      "Layer parameter\n",
      "De￿nition\n",
      "WI F M/OF M\n",
      "Width of the input/output feature map\n",
      "DI F M/OF M\n",
      "Depth of the input/output feature map\n",
      "Fcon�/pool\n",
      "Width of the conv/pooling ￿lter\n",
      "Scon�/pool\n",
      "Stride of the conv/pooling ￿lter\n",
      "Pcon�/pool\n",
      "Number of pixels padded in the conv/pooling layer\n",
      "P\n",
      "Indicate the existence of the pooling layer\n",
      "Table 2: Parameters to de￿ne a CNN structure.\n",
      "3.1\n",
      "Attack Methodology\n",
      "In order to construct a neural network, an adversary needs to know\n",
      "the number of layers, parameters for each layer, and connections\n",
      "among layers. A typical CNN uses a simple sequential connection\n",
      "only between consecutive layers. More recent proposal [] intro-\n",
      "duces an additional bypass connection among layers.\n",
      "We show that memory access patterns relatively easily reveal the\n",
      "overall layer structure through read-after-write (RAW) dependency.\n",
      "During the CNN inference, the RAW dependency on FMAPs must\n",
      "be preserved by the accelerator, regardless of its micro-architecture\n",
      "details and data reuse strategies. The OFMs are written by a pre-\n",
      "ceding layer and is read as the IFMs by the following layer. Since\n",
      "FMAPs are stored o￿-chip, this RAW dependency is re￿ected in\n",
      "the memory trace and visible to the adversary as a write followed\n",
      "by a read on the same memory address. These RAW dependencies\n",
      "can be used by the adversary to identify the boundary as well as\n",
      "the connections between layers. More concretely, the beginning of\n",
      "a new convolutional/fully connected layer is revealed by the ￿rst\n",
      "read access on a memory address that was previously written.\n",
      "Once the layer boundaries are identi￿ed, the adversary needs\n",
      "to further reverse engineer the key parameters of each layer. The\n",
      "￿rst step towards this goal is to distinguish memory accesses to\n",
      "￿lters, IFM, and OFM. Since the ￿lters are read-only and not up-\n",
      "dated during the inference, the adversary can di￿erentiate memory\n",
      "accesses to ￿lters from those accessing FMAPs. The read/write\n",
      "operations on IFM and OFM can also be distinguished since they\n",
      "have di￿erent access patterns. During the computation within a\n",
      "layer, memory locations holding OFM will only be written, typi-\n",
      "cally once. In contrast, if the adversary observes a read access on\n",
      "an address written in the previous layer before, this read must be\n",
      "for IFM. FMAPs and ￿lters are stored as arrays in memory, which\n",
      "means that each is stored in its own contiguous memory locations.\n",
      "Therefore, an adversary can infer the sizes of IFM (SIZEI F M), OFM\n",
      "(SIZEOF M), and ￿lters (SIZEF LT R) of each layer by observing the\n",
      "memory locations accessed for each data structure within a layer.\n",
      "So far, we have shown that the memory access pattern of a\n",
      "CNN accelerator reveals the number of layers, data dependencies\n",
      "(connections) among layers, the size of the IFM, OFM, and ￿lters\n",
      "for each layer in the target CNN model. However, not all operations\n",
      "\n",
      "in the neural network are explicitly revealed by the memory access\n",
      "pattern. For example, a CNN performs an activation operation\n",
      "after each convolution followed by an optional pooling operation.\n",
      "These three operations are often merged and performed together\n",
      "as a single layer in CNN accelerator for e￿ciency. As a result,\n",
      "the internal outputs of these three operations are invisible to the\n",
      "adversary.\n",
      "There are 11 structural parameters that the adversary needs\n",
      "to determine for each layer in order to fully de￿ne the network\n",
      "structure, as listed in Table 2. The problem of identifying each layer\n",
      "structure can be formulated as solving the 11 integer parameters\n",
      "with the following equations:\n",
      "SIZ EI F M = W \n",
      "I F M ⇥ DI F M\n",
      "(1)\n",
      "SIZ EO F M = W \n",
      "O F M ⇥ DO F M\n",
      "(2)\n",
      "SIZ EF LT R = F \n",
      "con� ⇥ DI F M ⇥ DO F M\n",
      "(3)\n",
      "WOF M =\n",
      "(WI F M �Fcon� +Pcon� )\n",
      "Scon�\n",
      "+ + (Ppool � Fpool ) ⇥ P\n",
      "Spool ⇥ P + ¯P\n",
      "+ P\n",
      "(4)\n",
      "Scon�  Fcon�  WI F M\n",
      "\n",
      "(5)\n",
      "Spool  Fpool  (WI F M � Fcon� + PI F M )\n",
      "Scon�\n",
      "+\n",
      "(6)\n",
      "Pcon� < Fcon�\n",
      "(7)\n",
      "Ppool < Fpool\n",
      "(8)\n",
      "Equations (1)-(3) are derived from the size of the FMAPs and\n",
      "weights revealed by the memory access patterns. Equation (4) ex-\n",
      "presses the relationship between the width of IFM and OFM. The\n",
      "inequalities are based on the following practical considerations.\n",
      "First, Fcon� and Fpool should be no less than the stride to cover\n",
      "all the pixels in IFM. Otherwise, some pixels in IFM are not con-\n",
      "nected with the weights and become redundant. Second, the ￿lters\n",
      "should be smaller compared to the width of IFM. Lastly, Pcon� and\n",
      "Ppool should be smaller than Fcon� and Fpool respectively because\n",
      "the convolutional/pooling ￿lter operating on zero-valued pixels is\n",
      "equivalent to adding zero-padding for the next layer.\n",
      "In our threat model, the adversary also has the knowledge of\n",
      "input and output of the accelerator which reveals the WI F M and\n",
      "DI F M of the ￿rst layer and the DOF M of the last layer. Although\n",
      "there is no previous write on the IFM of the ￿rst layer, the SIZEI F M\n",
      "can be calculated using Equation 1 and thus distinguished from the\n",
      "weights. Moreover, the WOF M of the last layer is one since there is\n",
      "exactly one score for each class. With these additional constraints,\n",
      "adversary can enumerate all possible parameters that satisfy Equa-\n",
      "tion (1)-(8) for the ￿rst layer and feed the possible WOF M and\n",
      "DOF M as the constraints for the second layer. Through enumerat-\n",
      "ing possible parameters layer-by-layer, the network structure is the\n",
      "combination of possible con￿gurations of each layer.\n",
      "In order to further reduce the number of possible structures,\n",
      "the execution time of each layer is measured by recording the\n",
      "number of clock cycles between the boundaries and introduced\n",
      "as an additional constraint. The number of MAC operations of a\n",
      "speci￿c layer can be computed with the layer parameters (# of MACs\n",
      "= W \n",
      "OF M ⇥ DOF M ⇥ Fcon� ⇥ DI F M). Given that the inference of\n",
      "most CNN models is compute-bound, we assume that the execution\n",
      "time is roughly proportional to the number of MAC operations.\n",
      "Thus, the execution time ratio between layers should be consistent\n",
      "with the ratio of MAC operations for the correct con￿guration.\n",
      "Once a small number of candidate structures are identi￿ed through\n",
      "the reverse engineering, an adversary can pick the best structure\n",
      "Networks\n",
      "LeNet\n",
      "ConvNet\n",
      "AlexNet\n",
      "SqueezeNet\n",
      "# of layers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# of possible structures\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table 3: Possible structures for di￿erent networks.\n",
      "Figure 3: Memory access pattern of the FPGA accelerator.\n",
      "by training and comparing the accuracy. Algorithm 1 summarizes\n",
      "the overall structure reverse-engineering attack procedure.\n",
      "Algorithm 1 Steps to uncover the possible CNN structures\n",
      "1: Identify layer boundaries by observing the RAW dependency\n",
      "on FMAPs\n",
      "2: Record the execution time of each layer and calculate the\n",
      "SIZEI F M, SIZEOF M, and SIZEF LT R based on the memory ac-\n",
      "cess pattern\n",
      "3: Find possible con￿gurations for each layer with the constraints\n",
      "stated in Equations (1)-(8)\n",
      "4: Filter out the con￿gurations where the number of MAC opera-\n",
      "tions and the execution time do not match\n",
      "5: List valid combination of layers as possible structure which\n",
      "satis￿es (WOF Mi = WI F Mi+) ^ (DOF Mi = DI F Mi+)\n",
      "3.2\n",
      "Case Studies\n",
      "To evaluate the proposed structure reverse-engineering attack,\n",
      "we performed case studies on popular CNN models: an 8-layer\n",
      "AlexNet [] and a more recent 18-layer SqueezeNet []. We also\n",
      "studied other smaller networks such as LeNet and ConvNet. The\n",
      "number of possible structures identi￿ed by the proposed attack is\n",
      "summarized in Table 3.\n",
      "We implemented an FPGA accelerator for AlexNet using Vivado\n",
      "HLS and performed the attack by inserting a hardware Trojan to\n",
      "collect the memory trace of the accelerator. The layer boundaries\n",
      "are identi￿ed by observing the RAW dependency on FMAPs, as de-\n",
      "picted in Figure 3. Table 4 lists the possible con￿gurations for each\n",
      "layer in AlexNet. A total of 24 valid combinations are uncovered\n",
      "by applying the proposed method on the FPGA prototype. A pool-\n",
      "ing layer, if exists, is considered as part of the convolutional layer.\n",
      "AlexNet consists of ￿ve CONV layers and three FC layers. The FC\n",
      "layers are not listed in the Table 4 because they use the largest\n",
      "possible ￿lter size (W \n",
      "I F M ⇥ DI F M ⇥ DOF M) and always have a\n",
      "unique con￿guration with respect to the Equations (1)-(8). The orig-\n",
      "inal AlexNet structure consists of CONV, CONV, CONV,\n",
      "CONV 4, and CONV. The top-1 validation accuracies of 24 possi-\n",
      "ble structures are shown in Figure 4. The original AlexNet achieves\n",
      "the fourth highest accuracy (57.3%). The attack also found three\n",
      "other network structure, which are slightly di￿erent from the orig-\n",
      "inal AlexNet and have higher accuracy. The best structure achieves\n",
      "12.3% higher accuracy than the worst one, showing the importance\n",
      "of a good network structure.\n",
      "\n",
      "La�er\n",
      "WI F M\n",
      "DI F M\n",
      "WOF M\n",
      "DOF M\n",
      "Fcon�\n",
      "Scon�\n",
      "Pcon�\n",
      "Fpool\n",
      "Spool\n",
      "Ppool\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table 4: Possible AlexNet layer con￿gurations – N/A indi-\n",
      "cates that there is no pooling operation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Possible AlexNet structures\n",
      "Top-1 validation accuracy (%)\n",
      "Other possible network structures\n",
      "Original AlexNet structure\n",
      "Figure 4: Top-1 validation accuracy among 24 possible struc-\n",
      "tures for AlexNet.\n",
      "In addition to AlexNet, we also studied reverse engineering of\n",
      "a more recent state-of-art CNN structure. Compared to AlexNet,\n",
      "two new trends emerged in network structure designs in the past\n",
      "few years. GoogLeNet [] proposed concatenating multiple con-\n",
      "volution ￿lters with di￿erent Fcon� as a module and using this\n",
      "module repeatedly to form the network. ResNet [] introduced a\n",
      "bypass connection between two non-adjacent layers. SqueezeNet\n",
      "adopted both of these structural changes and achieves an accuracy\n",
      "comparable to that of AlexNet while using 50x less weights.\n",
      "We use SqueezeNe as an example to demonstrate the e￿ective-\n",
      "ness of the proposed attack on a more recent network. SqueezeNet\n",
      "consists of two CONV layers and eight ￿re modules and each ￿re\n",
      "module is made of concatenating one 1x1 and two 3x3 convolutional\n",
      "￿lters. The 1x1 CONV layer uses small DOF M to squeeze the size of\n",
      "FMAPs. The OFM of the 1x1 CONV layer feeds into two 3x3 CONV\n",
      "layers in parallel and the OFMs of the two following layers are con-\n",
      "catenated along the depth dimension as the ￿nal OFM of the ￿re\n",
      "module. Compared to normal feed-through structure, SqueezeNet\n",
      "also introduces three bypass paths connecting non-adjacent ￿re\n",
      "modules. The bypass path is combined with the feed-through path\n",
      "by applying element-wise additions on the OFMs.\n",
      "To our best knowledge, there is no accelerator design that uses\n",
      "dedicated hardware for the ￿re module because it can be imple-\n",
      "mented using existing CONV layer accelerators. The three CONV\n",
      "layers will be executed sequentially. The IFM is convolved with 1x1\n",
      "CONV ￿lter ￿rst and the OFM of this 1x1 convolution is then con-\n",
      "voluted with 1x1 and 3x3 CONV ￿lters in series. Assuming that the\n",
      "three convolution operations are executed sequentially on a CNN\n",
      "accelerator, the adversary can observe the RAW dependency be-\n",
      "tween layers which reveals the structure of the ￿re module. Instead\n",
      "of having RAW dependency between the neighboring layers, the\n",
      "N ⇥N ￿lter is an abbreviation of N ⇥N ⇥DOF M ￿lter. And N ⇥N convolution\n",
      "stands for performing convolution with N ⇥ N ⇥ DOF M ￿lter.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "Figure 5: The top-5 validation accuracy of nine possible\n",
      "structures for SqueezeNet.\n",
      "bypass path introduces extra RAW dependency across non-adjacent\n",
      "layers. The easiest way of implementing this bypass function is\n",
      "to wait until two OFMs from two paths are both ready, then load\n",
      "them from memory and perform the element-wise additions. This\n",
      "method is adopted by Ca￿e and TensorFlow. In both frameworks, a\n",
      "separate element-wise layer is introduced to realize the bypass func-\n",
      "tion. Assuming that the CNN accelerator follows the same strategy,\n",
      "the bypass path can also be detected from the RAW dependency in\n",
      "memory accesses.\n",
      "We performed the proposed attack on SqueezeNet, and found\n",
      "that there are nine possible con￿gurations for CONV1 layer, 12\n",
      "possible con￿gurations for the ￿re modules, and two possible con-\n",
      "￿gurations for the CONV10 layer. Theoretically, there exists 329\n",
      "valid combinations, which makes it expensive to test all valid com-\n",
      "binations to identify the network structure. However, large CNNs\n",
      "are typically constructed in a modular fashion, where the same\n",
      "building block is reused in order to reduce the complexity. If we\n",
      "assume that the structures of all ￿re module are identical, there\n",
      "exists only one valid con￿guration for the ￿re module and CONV10\n",
      "layer. The number of possible CNN structure candidates is reduced\n",
      "to nine. This example shows that the number of possible structures\n",
      "does not necessarily grow exponentially with the number of layers.\n",
      "The time to search for the best network structure among the\n",
      "possible candidates can be reduced by using short training to\n",
      "quickly ￿lter out unpromising candidates with low classi￿cation\n",
      "accuracy. For example, CNN training is often performed using\n",
      "many epochs, where one epoch goes through the entire training\n",
      "dataset once. Figure 5 shows the accuracy of possible candidates for\n",
      "SqueezeNet when only three epochs were used for training. The\n",
      "original SqueezeNet proposed using around 70 epochs for training.\n",
      "There is a signi￿cant di￿erence in accuracy among the structure\n",
      "candidates even with a small number of epochs, suggesting that\n",
      "unpromising network structures can be quickly ￿ltered out.\n",
      "\n",
      "REVERSE ENGINEERING WEIGHTS\n",
      "EXPLOITING ZERO PRUNING\n",
      "In this section, we introduce an attach to obtain information on\n",
      "weights when an optimization technique is used to prune zeros in\n",
      "FMAPs. This attack also exploits information leaks through memory\n",
      "access patterns of the CNN accelerator. However, only the write\n",
      "accesses need to be observable for the attack.\n",
      "In recent years, multiple CNN accelerator designs observed that\n",
      "the ReLU function leads to a large number of zeros in the feature\n",
      "maps and proposed to dynamically prune those zeros [,,].\n",
      "\n",
      "(a) 1x1 CONV with stride 1.\n",
      "(b) n ⇥ n CONV with stride 1.\n",
      "Figure 6: 1x1 and n ⇥ n convolution.\n",
      "Such zero pruning technique is shown to be quite e￿ective, reduc-\n",
      "ing convolution operations by 40% on average without a￿ecting the\n",
      "classi￿cation accuracy. The zero pruning also reduces the number\n",
      "of memory accesses by only writing and reading non-zero values,\n",
      "for example using a run-length encoding. However, the dynamic\n",
      "zero pruning reveals the number of zeros in OFM. An adversary\n",
      "who can observe memory accesses can detect when the number\n",
      "of zeros changes. We show that this dynamic behavior leaks in-\n",
      "formation about the weights of the CNN model. The adversary\n",
      "cannot precisely recover all weights, but can recover substantial\n",
      "information so that each weight can be expressed as a function of\n",
      "one bias value.\n",
      "4.1\n",
      "Attack Methodology\n",
      "In a CNN accelerator, di￿erent inputs lead to a di￿erent number of\n",
      "non-zero pixels in the OFM of each layer. With the dynamic zero\n",
      "pruning, the adversary can observe the changes in the number of\n",
      "non-zero pixels in OFM. If the activation function f maps negative\n",
      "values to zero as in ReLU, the change in the number of zeros in OFM\n",
      "actually reveals when a pixel crosses the zero boundary. The value\n",
      "of each pixel� in OFM can be expressed as a function of the value of\n",
      "pixels x in IFM, weights w, and a bias b, that is � = f (Õ\n",
      "i wi ·xi +b).\n",
      "Therefore, if an adversary can slowly change the pixel values in\n",
      "IFM and observe the number of zeros in OFM, the adversary can\n",
      "e￿ectively ￿nd out when the value of a pixel in OFM becomes zero\n",
      "(Õ\n",
      "i wi ·xi +b = 0). If the adversary can ￿nd out which pixel is zero,\n",
      "then one can write a set of linear equations for wi and b given that\n",
      "the values of input xi are known.\n",
      "Unfortunately, the dynamic zero pruning only leaks the number\n",
      "of zero-valued pixels in OFM while the exact locations of those\n",
      "pixels along W , H, and D dimensions remain unknown. To solve\n",
      "the problem, an adversary can provide carefully crafted inputs to\n",
      "the accelerator. Reverse engineering the weights of 1x1 CONV, 2x2\n",
      "CONV, and FC layers are more straightforward compared to the\n",
      "general case with a larger ￿lter (Fcon� >). We ￿rst illustrate our\n",
      "approach using a special case when Fcon� = 1. Then, the approach\n",
      "is extended to work with any CONV ￿lter size.\n",
      "Figures 6a shows an example where a 3 ⇥ 3 IFM is convolved\n",
      "with a 1 ⇥ 1 ￿lter. In the 1x1 convolution, a single weight is shared\n",
      "by all WI F M ⇥ HI F M pixels on the same 2-D plane. In other words,\n",
      "the product of any pixel in IFM and the weight only a￿ect one pixel\n",
      "in the corresponding OFM. This relationship can be expressed by\n",
      "�i,j = xi,j ·w +b. Thus, for a 1x1 CONV layer, an adversary can vary\n",
      "the value of one speci￿c pixel in IFM (denoted as a variable x) and\n",
      "set all other pixels to be 0 as depicted in Figure 6a. By monitoring the\n",
      "number of non-zeros in OFM, the adversary can easily determine if\n",
      "w · x + b > 0 holds for a given x. Through a binary search on the\n",
      "value of x, the adversary will be able to ￿nd a maximum xH and a\n",
      "minimum xL, which satisfy:\n",
      "(w · xH + b) ^ (w · xL + b >)\n",
      "(9)\n",
      "Then (xH +xL)/2 can be estimated to be the input that produces\n",
      "zero (� = 0), which we subsequently refer to as a zero crossing\n",
      "point. It can be used to approximate the value of b/w.\n",
      "Figure 6b illustrates the general case of applying a n ⇥ n CONV\n",
      "￿lter on an IFM (WI F M >n). The number of connections be-\n",
      "tween each pixel and the weights is shown in the IFM. Having a\n",
      "connection with the weight wi,j means that pixel contributes to\n",
      "the corresponding output pixel �i,j. For instance, pixel x, at the\n",
      "top-left corner is only connected with w, and contributes to �,\n",
      "whereas pixel x, is connected with w, and w,, contributing\n",
      "to both �, and �,. xn,n, which is a pixel on the nth column and\n",
      "row, is connected with all n weights in the 2-D ￿lter.\n",
      "As the weights closer to the corner contributes to a less numbers\n",
      "of output pixels, we can iteratively ￿nd the ratio between each\n",
      "￿lter weight and the bias. For example, performing a binary search\n",
      "on x, reveals the approximate value of b/w,. Then, a search\n",
      "on x, while setting all other pixels to zero gives two regions\n",
      "where the number of zero-valued outputs changes between 0 and 2.\n",
      "Each one of these zero-crossing points is the approximate value of\n",
      "b/w, or b/w,. Given that b/w, is already known, an attacker\n",
      "can determine which value corresponds to b/w,. Algorithm 2\n",
      "describes the proposed method for discovering all b/wi,j on the\n",
      "same 2-D plane. Note that, if wi,j = 0, no x and x will satisfy\n",
      "Equation (9) which means no zero-crossing point can be found\n",
      "during the search. Therefore, zero-valued weights can be identi￿ed\n",
      "from missing zero-crossing points.\n",
      "Algorithm 2 Reverse engineering CNN weights\n",
      "1: for i = 0 to Fcon� � do\n",
      "2:\n",
      "for j = 0 to Fcon� � do\n",
      "3:\n",
      "Set all inputs except xi,j to zero.\n",
      "4:\n",
      "Find (xH + xL)/2 values where the number of non-zero\n",
      "output pixels change (zero crossing points).\n",
      "5:\n",
      "Set the new zero-crossing point as b/wi,j.\n",
      "6:\n",
      "end for\n",
      "7: end for\n",
      "A convolutional layer may be followed by a maximum or average\n",
      "pooling layer. These two layers are usually merged in a CNN ac-\n",
      "celerator to avoid unnecessary o￿-chip memory accesses. Without\n",
      "the pooling layer, varying x, while keeping other inputs to be\n",
      "zero leads to two zero-crossing points. However, when there is a\n",
      "max pooling layer where a 2x2 pooling window is concatenated\n",
      "with a n ⇥ n CONV layer, the search on x, only gives one zero-\n",
      "crossing point because both non-zero outputs are replaced by the\n",
      "same maximum value of w, · x, and w, · x,. If w, < w,,\n",
      "this zero-crossing point corresponds to b/w,. In order to also ￿nd\n",
      "b/w,, we need to keep both x, and x, as non-zero variables as\n",
      "in the following equation:\n",
      "(\n",
      "�, = max\n",
      "�\n",
      "w, · x, + w, · x,, w, · x,\n",
      " \n",
      "�, = w, · x, + b\n",
      "(10)\n",
      "Given that the b/w, has already been inferred, the attacker can\n",
      "￿nd the value for x, which satis￿es �,  0. Then performing\n",
      "binary search on x, to ￿nd the zero crossing point for w, ·\n",
      "The same bias is shared by all the weights in one ￿lter.\n",
      "\n",
      "Figure 7: Weight/Bias ratio of the CONV1 layer in AlexNet.\n",
      "x, + w, · x, + b gives\n",
      "b\n",
      "w,\n",
      "=\n",
      "�x,\n",
      " + x, ·w, /b . This modi￿ed\n",
      "algorithm can be generalized to be compatible with any k ⇥ k\n",
      "max pooling window. With a 2x2 average pooling layer, the same\n",
      "approach can be applied with the modi￿ed equation in Equation (11)\n",
      "and b/w, can be expressed with\n",
      "�bx,\n",
      "b + x, ·w,\n",
      ".\n",
      "(\n",
      "�, = w,·x,+b+w,·x,+b\n",
      "\n",
      "�, = w,·x,+b\n",
      "\n",
      "(11)\n",
      "The proposed attack on the dynamic zero pruning scheme al-\n",
      "lows each weight to be expressed as a function of the bias. This\n",
      "signi￿cantly reduces the entropy in weights as only the bias is left\n",
      "to be unknown. This unknown bias cannot be determined only\n",
      "from the information leak through the number of non-zero pixels.\n",
      "The changes in the number of non-zero pixels in OFM provides\n",
      "n unique equalities, one for each pixel for a n ⇥ n convolutional\n",
      "￿lter. Yet, there exists n + 1 variables for the ￿lter (n weights\n",
      "and one bias value). In order to determine the exact weights and\n",
      "the bias, an adversary needs to leverage additional information.\n",
      "For example, recent accelerator designs [,] proposed to use a\n",
      "tunable threshold function in place of the ReLU function in order to\n",
      "prune more pixels with small values and thus improves e￿ciency.\n",
      "However, if this non-zero threshold value is known and can be\n",
      "adjusted, an adversary can set the input to be all zeros and vary\n",
      "the threshold to ￿nd the bias values. Since the ratio between each\n",
      "weight and bias is known, this optimization enables an adversary\n",
      "to fully recover the weight and bias values.\n",
      "4.2\n",
      "Case Study\n",
      "We demonstrate the proposed attack on the ￿rst layer of a com-\n",
      "pressed AlexNet model [], which contains zero-valued weights.\n",
      "The inferred w/b of all 96 ￿lters are shown in Figure 7. The zero-\n",
      "valued weights are detected and the maximum di￿erence between\n",
      "the inferred and the original ratio is less than 2�.\n",
      "\n",
      "RELATED WORK\n",
      "While this work represents the ￿rst concrete study on exploiting\n",
      "information leaks through memory access patterns in the context\n",
      "of reverse engineering CNNs, hiding information leaks through\n",
      "memory access patterns in general is a well-studied problem. In\n",
      "particular, oblivious RAM (ORAM) algorithms [] provide a strong\n",
      "theoretical guarantee for obfuscating the memory accesses. ORAM\n",
      "can be used to prevent attacks proposed in this paper. However,\n",
      "even an e￿cient hardware implementation [] of the state-of-the-\n",
      "art ORAM algorithm [] signi￿cantly increases the number of\n",
      "memory accesses, and likely to result in signi￿cant overhead for\n",
      "the CNN inference, which is a memory-intensive task.\n",
      "Membership inference attack [] proposed constructing shadow\n",
      "models to identify whether a input belongs to the original training\n",
      "dataset or not. This attack relies on training the shadow models\n",
      "when the network structure is already known. The proposed struc-\n",
      "ture reverse-engineering attack can be used to enable such attacks.\n",
      "\n",
      "CONCLUSION\n",
      "This paper studies potential vulnerabilities in CNN accelerators in\n",
      "the context of stealing a CNN model. The study shows that both\n",
      "the network structure and weights of a CNN model can be revealed\n",
      "through the memory access patterns and the input/output of the\n",
      "accelerator even when no internal access is allowed and all o￿-\n",
      "chip data are encrypted. Our ￿ndings highlight the need for hiding\n",
      "memory access patterns for CNN accelerators. The study also shows\n",
      "that performance optimization can lead to an unexpected security\n",
      "vulnerability and needs to be carefully reviewed.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "This work was partially sponsored by NSF award CNS-1618275,\n",
      "Semiconductor Research Corporation under Task 2686.001, and a\n",
      "GPU donation from NVIDIA Corporation.\n",
      "REFERENCES\n",
      "[1] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright\n",
      "Jerger, and Andreas Moshovos. Cnvlutin: Ine￿ectual-neuron-free Deep Neural\n",
      "Network Computing. In ISCA, 2016.\n",
      "[2] Victor Costan and Srinivas Devadas. Intel SGX Explained. IACR Cryptology\n",
      "ePrint Archive, 2016.\n",
      "[3] Christopher W. Fletcher, Ling Ren, Albert Kwon, Marten van Dijk, Emil Stefanov,\n",
      "Dimitrios Serpanos, and Srinivas Devadas. A Low-Latency, Low-Area Hardware\n",
      "Oblivious RAM Controller. In FCCM, 2015.\n",
      "[4] Oded Goldreich and Rafail Ostrovsky. Software Protection and Simulation on\n",
      "Oblivious RAMs. J. ACM, 1996.\n",
      "[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
      "Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets.\n",
      "In NIPS. 2014.\n",
      "[6] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing\n",
      "Deep Neural Network with Pruning, Trained Quantization and Hu￿man Coding.\n",
      "CoRR, 2015.\n",
      "[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning\n",
      "for Image Recognition. CoRR, 2015.\n",
      "[8] Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J.\n",
      "Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer\n",
      "parameters and <1MB model size. CoRR, 2016.\n",
      "[9] Alex Krizhevsky, Ilya Sutskever, and Geo￿rey E Hinton. ImageNet Classi￿cation\n",
      "with Deep Convolutional Neural Networks. In NIPS. 2012.\n",
      "[10] Lichun Li and Anwitaman Datta. Write-only Oblivious RAM-based Privacy-\n",
      "preserved Access of Outsourced Data. Int. J. Inf. Secur., 2017.\n",
      "[11] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rang-\n",
      "harajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and\n",
      "William J. Dally. SCNN: An Accelerator for Compressed-sparse Convolutional\n",
      "Neural Networks. In ISCA, 2017.\n",
      "[12] B. Reagen, P. Whatmough, R. Adolf, S. Rama, H. Lee, S. K. Lee, J. M. HernÃąndez-\n",
      "Lobato, G. Y. Wei, and D. Brooks. Minerva: Enabling Low-Power, Highly-Accurate\n",
      "Deep Neural Network Accelerators. In ISCA, 2016.\n",
      "[13] Reza Shokri and Vitaly Shmatikov. Privacy-Preserving Deep Learning. In Pro-\n",
      "ceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications\n",
      "Security, CCS, 2015.\n",
      "[14] Reza Shokri, Marco Stronati, and Vitaly Shmatikov. Membership Inference\n",
      "Attacks against Machine Learning Models. CoRR, 2016.\n",
      "[15] Emil Stefanov, Marten van Dijk, Elaine Shi, Christopher Fletcher, Ling Ren,\n",
      "Xiangyao Yu, and Srinivas Devadas. Path ORAM: An Extremely Simple Oblivious\n",
      "RAM Protocol. In CCS, 2013.\n",
      "[16] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,\n",
      "Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabi-\n",
      "novich. Going Deeper with Convolutions. CoRR, 2014.\n",
      "[17] Yuanzhong Xu, Weidong Cui, and Marcus Peinado. Controlled-Channel Attacks:\n",
      "Deterministic Side Channels for Untrusted Operating Systems. In S&P, 2015.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(allText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n",
      "Reverse Engineering Convolutional Neural Networks Through\n",
      "Side-channel Information Leaks\n",
      "Weizhe Hua, Zhiru Zhang, and G. Edward Suh\n",
      "School of Electrical and Computer Engineering, Cornell University, Ithaca, NY\n",
      "{wh399, zhiruz, gs272}@cornell.edu\n",
      "ABSTRACT\n",
      "A convolutional neural network (CNN) model represents a crucial\n",
      "piece of intellectual property in many applications. Revealing its\n",
      "structure or weights would leak con￿dential information. In this pa-\n",
      "per we present novel reverse-engineering attacks on CNNs running\n",
      "on a hardware accelerator, where an adversary can feed inputs to\n",
      "the accelerator and observe the resulting o￿-chip memory accesses.\n",
      "Our study shows that even with data encryption, the adversary can\n",
      "infer the underlying network structure by exploiting the memory\n",
      "and timing side-channels. We further identify the information leak-\n",
      "age on the values of weights when a CNN accelerator performs\n",
      "dynamic zero pruning for o￿-chip memory accesses. Overall, this\n",
      "work reveals the importance of hiding o￿-chip memory access\n",
      "pattern to truly protect con￿dential CNN models.\n",
      "\n",
      "INTRODUCTION\n",
      "Convolutional neural networks (CNNs) are quickly becoming an\n",
      "essential tool in a wide range of machine learning applications.\n",
      "In many application scenarios, CNN models — both its network\n",
      "structure and learnable parameters (i.e., weights) — need to be\n",
      "protected as con￿dential information: (1) for companies that rely\n",
      "on a CNN to provide a core or value-added service, the underlying\n",
      "neural network model represents an important piece of intellectual\n",
      "property; (2) in personalized applications such as digital assistants,\n",
      "CNN models are trained using private data, and the weights need to\n",
      "be kept con￿dential for privacy []; (3) furthermore, recent studies\n",
      "on the adversarial network show that an attacker can intentionally\n",
      "a￿ect the outcome of CNN-based classi￿cation and object detection\n",
      "by perturbing input images when the network model is known [].\n",
      "This paper investigates reverse-engineering attacks on CNN\n",
      "models exploiting information leaks through memory and timing\n",
      "side-channels. Speci￿cally, we study attacks on a hardware acceler-\n",
      "ator that is protected by secure processor techniques similar to the\n",
      "scheme used in Intel SGX []. In this setting, an adversary can feed\n",
      "inputs to a protected computation and observe o￿-chip accesses,\n",
      "but cannot observe or change the computation and the internal\n",
      "state. Surprisingly, we show that an adversary can e￿ectively re-\n",
      "verse engineer both the structure and the weights of an encrypted\n",
      "CNN model running on a hardware accelerator that performs the in-\n",
      "ference (i.e., forward propagation). Because the CNN states (feature\n",
      "maps) and parameters (weights) are often quite large, it is impracti-\n",
      "cal to hold all feature maps, weights, and intermediate results in the\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for pro￿t or commercial advantage and that copies bear this notice and the full citation\n",
      "on the ￿rst page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "DAC ’18, June 24–29, 2018, San Francisco, CA, USA\n",
      "© 2018 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-5700-5/18/06...$15.00\n",
      "https://doi.org/https://doi.org/10.1145/3195970.3196105\n",
      "Figure 1: A typical CNN inference accelerator.\n",
      "on-chip memory of an accelerator. As a result, CNN accelerators\n",
      "typically store feature maps and weights in o￿-chip memory and\n",
      "access them as needed. Even if data values are encrypted, memory\n",
      "access patterns reveal which memory locations are accessed and\n",
      "whether each access is a read or a write. In this study, we show\n",
      "that the memory access patterns expose key parameters of the net-\n",
      "work structure such as the number of layers, input/output sizes\n",
      "of each layer, the size of ￿lters, data dependencies among layers,\n",
      "etc. Given this information, an attacker can infer a small set of\n",
      "possible network structures by further considering the execution\n",
      "time of a CNN accelerator, which indicates the amount of computa-\n",
      "tion. In our experiments, we demonstrate the proposed attack by\n",
      "reversing engineering the structures of two popular CNN models\n",
      "in AlexNet [9] and SqueezeNet [8].\n",
      "In addition to revealing the network structure, this study shows\n",
      "that the memory access patterns also leak information on weight\n",
      "values when dynamic zero pruning is used for o￿-chip memory\n",
      "accesses. The optimization is based on the observation that the\n",
      "feature maps from the intermediate layers of a CN\n"
     ]
    }
   ],
   "source": [
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pdf_audio)",
   "language": "python",
   "name": "pdf_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
